Q1) Recall from the video the structure of one of the simplest neural networks,
https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/JKIDlfihEeeAPQprC3K2Bg_66921c651e175da7cc8b805860da4225_simple.png?expiry=1552176000000&hmac=gWCMyUZEDQ1fFXMeEabwNukvaRW8EIdt8uZLpRtQC1E
Here there are only two neurons (or nodes), and they are linked by a single edge.
The activation of neurons in the final layer, (1), is determined by the activation of neurons in the previous layer, (0),
a^(1)= σ(w^(1) . a^(0) + b^(1)),
where w^(1) is the weight of the connection between Neuron (0) and Neuron (1), and b^(1) is the bias of the Neuron (1). These are then subject to the activation function, σ to give the activation of Neuron (1)
Our small neural network won't be able to do a lot - it's far too simple. It is however worth plugging a few numbers into it to get a feel for the parts.
Let's assume we want to train the network to give a NOT function, that is if you input 1 it returns 0, and if you input 0 it returns 1.
For simplicity, let's use, σ(z)=tanh(z), for our activation function, and randomly initialise our weight and bias to w^(1)=1.3 and b^(1)=−0.1.
Use the code block below to see what output values the neural network initially returns for training data.
# First we set the state of the network
σ = np.tanh
w1 = 1.3
b1 = -0.1

# Then we define the neuron activation.
def a1(a0) :
  return σ(w1 * a0 + b1)
  
# Finally let's try the network out!
# Replace x with 0 or 1 below,
a1(x)
It's not very good! But it's not trained yet; experiment by changing the weight and bias and see what happens.
Choose the weight and bias that gives the best result for a NOT function out of all the options presented.
A1) w^(1)=−5, b^(1)=5

Q2) Let's extend our simple network to include more neurons.
https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/BmRF4RgTEei0zRLQ2V54Gg_f661933d95252c6e3d84e779bde7c119_moreNodes.png?expiry=1552176000000&hmac=3YVNQGdY0L61yGOUH8czk2YAsk0reSAcX8pF56LJC6A
We now have a slightly changed notation. The neurons which are labelled by their layer with a superscript in brackets, are now also labelled with their number in that layer as a subscript, and form vectors a^(0) and a^(1).
The weights now form a matrix W^(1), where each element, w^{(1)}_{ij}, is the link between the neuron j in the previous layer and neuron i in the current layer. For example w^{(1)}_{12} is highlighted linking a^{(0)}_2 to a^{(1)}_1.
The biases similarly form a vector b^{(1)}.
We can update our activation function to give,
a^(1)= σ(W^(1) . a^(0) + b^(1)),
where all the quantities of interest have been upgraded to their vector and matrix form and σ acts upon each element of the resulting weighted sum vector separately.
For a network with weights, W^(1) = [[-2, 4, -1], [6, 0, -3]], and bias b=[[0.1], [-2.5]], calculate the output, a^{(1)}, given an input vector, a^{(0)} = [[0.3], [0.4], [0.1]]
You may do this calculation either by hand (to 2 decimal places), or by writing python code. Input your answer into the code block below.
(If you chose to code, remember that you can use the @ operator in Python to perform operate a matrix on a vector.)
A2) a1 = np.array([0.76, -0.76])

Q3) Now let's look at a network with a hidden layer.
https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/lO8V1vlhEeetIhICvTHYsg_3953370b0690ae02c046d9a903540998_moreBoth.png?expiry=1552176000000&hmac=XdgrVUuiKpnJT21fcjdTvmmMIspiWUprRZCb_XmDqUY
Here, data is input at layer (0), this activates neurons in layer (1), which become the inputs for neurons in layer (2).
(We've stopped explicitly drawing the biases here.)
Which of the following statements are true?
A3) - This neural network has 5 biases.
    - The number of weights in a layer is the product of the input and output neurons to that layer.

Q4) Which of the following statements about the neural network from the previous question are true?
A4) a^(2)=σ( W^(2) σ(W^(1) a^(0)+b^(1))+b^(2) ),

Q5) So far, we have concentrated mainly on the structure of neural networks, let's look a bit closer at the function, and what the parts actually do.
We'll introduce another network, this time with a one dimensional input, a one dimensional output, and a hidden layer with two neurons.
Use the tool below to change the values of the four weights and three biases, and observe what effect this has on the network's function.
With the weights and biases set here, observe how a^{(1)}_0 activates when a^{(0)}_0 is active, and a^{(1)}_1 activates when a^{(0)}_0 is inactive. Then the output neuron, a^{(2)}_0, activates when neither a^{(1)}_0 nor a^{(1)}_1 are too active.
(Interact with the plugin below to score the point for this question.)
A5) Just Interact